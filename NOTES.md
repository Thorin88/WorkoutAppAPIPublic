### GCP

Download the command line toolkit at https://cloud.google.com/sdk/docs/install

Run gcloud init

gcloud config set project project-name

gcloud auth login

gcloud auth application-default login

gcloud auth configure-docker

Need to make a GCP database instance on GCP for the GCP version to use.

SQL -> new instance

Make secret for db password. The secret should be in the form username:password so it can be used directly in the db URLs.

No backups, minimal storage, lowest options possible

Setup so env file determines the db connections to form, local, all cloud, partial cloud. Since cloud run wants different connection URLs.

Need google cloud proxy

https://cloud.google.com/sql/docs/mysql/connect-auth-proxy#tcp-sockets

./cloud-sql-proxy --address 0.0.0.0 --port 1234 INSTANCE_CONNECTION_NAME

./cloud-sql-proxy --address 0.0.0.0 --port 5436 practice-project-thorin:europe-west2:practice-databases

The docker container needs credentials copied over into it. This is done via a volume in the docker-compose file.

Since the container is being built locally, the GOOGLE_APPLICATION_CREDENTIALS env variable is not set. This happens
automatically when the container is deployed to cloud run with an associated service account. Therefore for locally run containers,
this variable needs to be set.

- TODO -> Generate a service account via a script, then on runs of the container the container grabs the file generated by this script.

Generating a password with GCP isn't always safe, as @ in a password will mess the URL up.

The local runs also need the SQL proxy running locally, and then the container is looking to read from the port that is mapped to it's internal ports.

~~- TODO -> Have all env files use yaml formatting~~

Make scripts to make service accounts, with different service accounts and secret access per environment

Can deploy in isolation, or use cloud build to deploy with some parameters

Trigger will prompt you to manually allow GCP to use the github repo. Also need to install GCP build for the repo you want to use this for.

You can see logs of the trigger builds by going to cloud build and clicking the build ID.

Cloud build uses a standalone service account to make and deploy builds. This SA needs the role roles/run.developer for cloud build to work on cloud run deployments. This SA can be edited at Cloud Build/Settings.

Builds execution details show you the values of the substitution variables, but won't show you if they are being used/used correctly.

~~TODO -> cloud build needs to use no auth flag~~

~~TODO -> connect dev to dif db~~

Can confirm using DBeaver which DB user owns tables, to check the correct user was used by the deployments in their environments.

TODO -> Python code that submits the request
- use notebook + environment
- Notes on how bear token's work
- Making a payload

TODO -> Look at examples of how models were used
- Traffic changing
- Model version detection/usage

TODO -> Have flask take note of the environment
- It does via env variables, but not via Config objects themselves, which is fine.

The CI/CD builds are initialised using a specific comment that needs to be made during PRs. This allows comments to be addressed on the PR before anything is built.

### Databases

workout-app-db
workout-app-db-dev

workout-app-api-user
workout-app-api-user-dev

Uses fixed secrets, so uses passwords and usernames in the secrets, which means none of these are hardcoded, or even variables, in the code itself.

Databases can be reduced in cost by stopping the instance on GCP, comparing to deleting it.

### Resources to monitor

- Artifact Repository
- Cloud Run
- SQL
- Cloud Build
- Cloud Build Triggers

### Flask

To avoid issues with circular imports, it may be best to have the functions for a route just call a non-wrapped function for the route.